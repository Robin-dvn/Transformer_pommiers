{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook qui compare architecture, learning rate....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison taille de transformer\n",
    "Comparaison de 3 taills de transformer pour garder le moins lourd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformer import Transformer\n",
    "from PommierDataset import PommierDataset, collate_fn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"out/sequence_analysis_generated_dataset10000.csv\"\n",
    "dataset = PommierDataset(dataset_path)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "VAL_SPLIT = 0.8\n",
    "train_size = int(VAL_SPLIT * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_split, val_split = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 512\n",
    "train_loader = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_split, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# List of transformer sizes\n",
    "transformer_sizes = [16, 32, 64, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Iterate over transformer sizes\n",
    "for size in transformer_sizes:\n",
    "    NUM_EPOCHS = 12\n",
    "    LEARNING_RATE = 0.00005\n",
    "    MAX_D_MODEL = 128\n",
    "\n",
    "    num_heads = max(1, size // 32)  # Ensure at least 1 head\n",
    "    exp_name = f\"Transformer_{size}\"\n",
    "    wandb.init(\n",
    "        project=\"Topologie-Pommiers\",\n",
    "        name=exp_name,\n",
    "        config={\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"Val split\": VAL_SPLIT,\n",
    "            \"architecture\": exp_name,\n",
    "            \"dataset\": \"sequence_analysis_generated_dataset10000.csv\",\n",
    "            \"batch size\": BATCH_SIZE,\n",
    "            \"Dimension model\": size,\n",
    "            \"Number of heads\": num_heads,\n",
    "            \"epochs\": NUM_EPOCHS,\n",
    "            \"dynamic\": False\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Transformer(17, 12, size, num_heads, 0)\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Add number of parameters to wandb config\n",
    "    wandb.config.update({\"num_parameters\": sum(p.numel() for p in model.parameters())})\n",
    "\n",
    "    # Initialize the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for enc_inp, dec_inp, dec_target in train_loader:\n",
    "            enc_inp = enc_inp.to(model.device)\n",
    "            dec_inp = dec_inp.to(model.device)\n",
    "            dec_target = dec_target.to(model.device)\n",
    "            padding_mask = (dec_inp == 0).to(torch.float32)\n",
    "\n",
    "            logits = model(enc_inp, dec_inp, padding_mask)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            dec_target = dec_target.view(-1)\n",
    "            loss = criterion(logits, dec_target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for enc_inp, dec_inp, dec_target in val_loader:\n",
    "                enc_inp = enc_inp.to(model.device)\n",
    "                dec_inp = dec_inp.to(model.device)\n",
    "                dec_target = dec_target.to(model.device)\n",
    "                padding_mask = (dec_inp == 0).to(torch.float32).to(model.device)\n",
    "\n",
    "                logits = model(enc_inp, dec_inp, padding_mask)\n",
    "                logits = logits.view(-1, logits.size(-1))\n",
    "                dec_target = dec_target.view(-1)\n",
    "                loss = criterion(logits, dec_target)\n",
    "                total_eval_loss += loss.item()\n",
    "                wandb.log({\"val_loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_eval_loss / len(val_loader)\n",
    "        print(f\"[INFO] Epoch {epoch} : train loss = {avg_train_loss:.4f}, val loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"transformer_{size}.pth\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pommiers (Python 3.13.0)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Robin/Documents/Stage pommiers/Transformer_pommiers/pommiers/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Comparaison du nombre de couches de transformateur\n",
    "Dans cette section, nous allons comparer les performances des transformateurs en fonction du nombre de couches. Nous allons utiliser une taille de transformateur fixe et faire varier le nombre de couches pour observer l'impact sur les performances du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of number of layers\n",
    "num_layers_list = [1, 2, 3, 6]\n",
    "fixed_size = 32\n",
    "num_heads = 1\n",
    "\n",
    "# Iterate over number of layers\n",
    "for num_layers in num_layers_list:\n",
    "    NUM_EPOCHS = 12\n",
    "    LEARNING_RATE = 0.00005\n",
    "\n",
    "    exp_name = f\"Transformer_{fixed_size}_layers_{num_layers}\"\n",
    "    wandb.init(\n",
    "        project=\"Topologie-Pommiers\",\n",
    "        name=exp_name,\n",
    "        config={\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"Val split\": VAL_SPLIT,\n",
    "            \"architecture\": exp_name,\n",
    "            \"dataset\": \"sequence_analysis_generated_dataset10000.csv\",\n",
    "            \"batch size\": BATCH_SIZE,\n",
    "            \"Dimension model\": fixed_size,\n",
    "            \"Number of heads\": num_heads,\n",
    "            \"Number of layers\": num_layers,\n",
    "            \"epochs\": NUM_EPOCHS,\n",
    "            \"dynamic\": False\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Transformer(17, num_layers, fixed_size, num_heads, 0)\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Add number of parameters to wandb config\n",
    "    wandb.config.update({\"num_parameters\": sum(p.numel() for p in model.parameters())})\n",
    "\n",
    "    # Initialize the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for enc_inp, dec_inp, dec_target in train_loader:\n",
    "            enc_inp = enc_inp.to(model.device)\n",
    "            dec_inp = dec_inp.to(model.device)\n",
    "            dec_target = dec_target.to(model.device)\n",
    "            padding_mask = (dec_inp == 0).to(torch.float32)\n",
    "\n",
    "            logits = model(enc_inp, dec_inp, padding_mask)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            dec_target = dec_target.view(-1)\n",
    "            loss = criterion(logits, dec_target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for enc_inp, dec_inp, dec_target in val_loader:\n",
    "                enc_inp = enc_inp.to(model.device)\n",
    "                dec_inp = dec_inp.to(model.device)\n",
    "                dec_target = dec_target.to(model.device)\n",
    "                padding_mask = (dec_inp == 0).to(torch.float32).to(model.device)\n",
    "\n",
    "                logits = model(enc_inp, dec_inp, padding_mask)\n",
    "                logits = logits.view(-1, logits.size(-1))\n",
    "                dec_target = dec_target.view(-1)\n",
    "                loss = criterion(logits, dec_target)\n",
    "                total_eval_loss += loss.item()\n",
    "                wandb.log({\"val_loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_eval_loss / len(val_loader)\n",
    "        print(f\"[INFO] Epoch {epoch} : train loss = {avg_train_loss:.4f}, val loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"transformer_{fixed_size}_layers_{num_layers}.pth\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pommiers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
